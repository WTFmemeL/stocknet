#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# run_experiments_v17.py - Target adaptatif : Top 20% des mouvements

import os
import sys
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks, Model
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight

# Ajouter les chemins pour les imports locaux
script_dir = os.path.dirname(os.path.abspath(__file__))
code_dir = os.path.join(script_dir, "stocknet-code")
sys.path.insert(0, script_dir)
sys.path.insert(0, code_dir)

# Imports conditionnels des modules locaux
try:
    from vol_price_ratio import calculate_vol_price_ratio
    from ctx_div_cvd_uptrend import calculate_ctx_div_cvd_uptrend
    from ctx_div_rsi_downtrend import calculate_ctx_div_rsi_downtrend
    from ctx_div_cvd_lowvol import calculate_ctx_div_cvd_lowvol
    from ctx_div_rsi_lowvol import calculate_ctx_div_rsi_lowvol
    from cvd_state import calculate_cvd_state
    MODULES_OK = True
except ImportError as e:
    print(f"âš ï¸ Modules locaux non trouvÃ©s: {e}")
    MODULES_OK = False

# --- CONFIG CPU ONLY ---
os.environ["CUDA_VISIBLE_DEVICES"] = ""
tf.get_logger().setLevel("ERROR")

print("\n=== StockNet v17 â€“ Target Adaptatif (Top 20% des mouvements) ===")

if not MODULES_OK:
    print("\nâŒ Impossible de continuer sans les modules requis.")
    sys.exit(1)

# --- CHARGEMENT DES DONNÃ‰ES ---
df = pd.read_parquet("/home/jx/Documents/stocknet/out/features/BTC_features_v9.parquet")
df = calculate_vol_price_ratio(df)
df = calculate_ctx_div_cvd_uptrend(df)
df = calculate_ctx_div_rsi_downtrend(df)
df = calculate_ctx_div_cvd_lowvol(df)
df = calculate_ctx_div_rsi_lowvol(df)
print(f"âœ… DonnÃ©es : {len(df):,} lignes")

# === INTÃ‰GRATION AUTOMATIQUE DU CVD_STATE ===
df = calculate_cvd_state(df)
print("âœ… Colonne cvd_state gÃ©nÃ©rÃ©e.")

# === AJOUT FEATURE : RÃ‰GIME DE MARCHÃ‰ ===
print("\nðŸŽ¯ Calcul du rÃ©gime de marchÃ©...")

# VolatilitÃ© roulante
rolling_vol = df["close"].pct_change().rolling(50).std()

# Classification en 3 rÃ©gimes
vol_q33 = rolling_vol.quantile(0.33)
vol_q66 = rolling_vol.quantile(0.66)

df["market_regime"] = np.where(
    rolling_vol > vol_q66, 2,      # Haute volatilitÃ© (trending/breakout)
    np.where(rolling_vol < vol_q33, 0, 1)  # Basse volatilitÃ© (range) / Moyen
)

print(f"âœ… RÃ©gimes : Low={vol_q33:.6f}, Mid, High={vol_q66:.6f}")
print(f"Distribution : {df['market_regime'].value_counts().sort_index().to_dict()}")

# === TARGET ADAPTATIF : TOP 20% DES MOUVEMENTS ===
print("\nðŸŽ¯ GÃ©nÃ©ration du target adaptatif (Top 20%)...")
print("   Principe : Les 20% meilleurs mouvements haussiers/baissiers")
print("   FenÃªtre : 1-12 minutes")
print("   Avantage : Distribution 20/60/20 plus Ã©quilibrÃ©e")

window = 12  # minutes

# 1. Calcul des returns futurs sur la fenÃªtre
future_returns = pd.DataFrame({
    f'ret_{i}': df["close"].pct_change(i).shift(-i) 
    for i in range(1, window + 1)
})

# 2. Maximum et minimum des mouvements sur la fenÃªtre
max_future_move = future_returns.max(axis=1)
min_future_move = future_returns.min(axis=1)

# 3. CLASSIFICATION ADAPTATIVE PAR QUANTILES
# Top 20% des mouvements haussiers = Strong Buy
# Top 20% des mouvements baissiers = Strong Sell
# Le reste = Hold

quantile_buy = max_future_move.quantile(0.80)   # 80Ã¨me percentile
quantile_sell = min_future_move.quantile(0.20)  # 20Ã¨me percentile

print(f"\nðŸ“Š Seuils adaptatifs calculÃ©s :")
print(f"   Buy threshold (top 20%) : {quantile_buy:.4%}")
print(f"   Sell threshold (bottom 20%) : {quantile_sell:.4%}")

# Classification finale
df["target"] = np.where(
    max_future_move >= quantile_buy, 2,      # Strong Buy (top 15%)
    np.where(min_future_move <= quantile_sell, 0, 1)  # Strong Sell (bottom 15%) / Hold
)

# Stockage des mÃ©triques pour analyse
df["max_future_move"] = max_future_move
df["min_future_move"] = min_future_move

df = df.dropna()

# --- DIAGNOSTIC DE DISTRIBUTION ---
print("\nðŸ“Š Distribution des classes (avant split) :")
print(df["target"].value_counts().sort_index())
print("\nProportions :")
print(df["target"].value_counts(normalize=True).sort_index())

# Statistiques dÃ©taillÃ©es
print(f"\nðŸ“ˆ Statistiques des mouvements :")
print(f"   Max moyen : {max_future_move.mean():.4%}")
print(f"   Min moyen : {min_future_move.mean():.4%}")
print(f"   Max mÃ©dian : {max_future_move.median():.4%}")
print(f"   Min mÃ©dian : {min_future_move.median():.4%}")
buy_mask = df['target'] == 2
sell_mask = df['target'] == 0
hold_mask = df['target'] == 1
print(f"\n   Mouvement moyen des Strong Buy : {df.loc[buy_mask, 'max_future_move'].mean():.4%}")
print(f"   Mouvement moyen des Strong Sell : {df.loc[sell_mask, 'min_future_move'].mean():.4%}")
print(f"   Mouvement moyen des Hold : {df.loc[hold_mask, 'max_future_move'].mean():.4%}")

# --- VÃ‰RIFICATION ANTI-LEAKAGE RENFORCÃ‰E ---
print("\nðŸ” === VÃ‰RIFICATION ANTI-LEAKAGE ===")
print("\nÃ‰chantillon de donnÃ©es (indices 1000-1005) :")
check_cols = ["close", "ret", "rsi_14", "max_future_move", "target"]
print(df[check_cols].iloc[1000:1005])

# --- LISTE DES FEATURES (avec rÃ©gime de marchÃ©) ---
features = [
    "ret","rsi_14","ema_20","ema_50","volatility_zscore","trend_state",
    "vol_state","ctx_confluence","hour_sin","hour_cos",
    "delta_price","delta_vol","ofi_proxy","cvd_proxy","delta_power",
    "spread","price_change","vol_imbalance","volatility_20","cvd_state",
    "market_regime"
]

print(f"\nâœ… {len(features)} features utilisÃ©es")
print("   Note : Le volume est utilisÃ© comme feature (delta_vol, vol_imbalance)")
print("   â†’ Le modÃ¨le apprend lui-mÃªme son importance !")

# --- SHIFTING ANTI-LEAK STRICT ---
print("\nðŸ”’ Application du shifting anti-leak...")
for f in features:
    if f in df.columns:
        df[f] = df[f].shift(1)
    else:
        print(f"âš ï¸ Feature manquante ignorÃ©e: {f}")
df = df.dropna()

# VÃ©rification post-shift
print("\nðŸ” AprÃ¨s shifting (indices 1000-1005) :")
print(df[check_cols].iloc[1000:1005])

# --- NORMALISATION ---
if "cvd_state" in df.columns:
    mapping = {"Weak": -1, "Neutral": 0, "Strong": 1}
    df["cvd_state"] = df["cvd_state"].map(mapping)
    print("\nâœ… cvd_state encodÃ© numÃ©riquement.")

scaler = StandardScaler()
df[features] = scaler.fit_transform(df[features])

# --- SPLIT TRAIN / TEST ---
split_idx = int(len(df) * 0.75)
train, test = df.iloc[:split_idx], df.iloc[split_idx:]
gap = int(len(df) * 0.05)
test = test.iloc[gap:]

print(f"\nâ±ï¸ Split strict: train={len(train)}, test={len(test)}, gap={gap}")

# --- FENÃŠTRAGE GLISSANT ---
def make_windows(data, horizon=100, stride=10):
    X, y = [], []
    vals, labels = data[features].values, data["target"].values
    for i in range(0, len(vals)-horizon-1, stride):
        X.append(vals[i:i+horizon])
        y.append(labels[i+horizon])
    return np.array(X), np.array(y)

print("\nðŸ”„ FenÃªtrage glissant avec stride=10...")
X_train, y_train = make_windows(train, stride=10)
X_test, y_test = make_windows(test, stride=10)

print(f"âœ… FenÃªtrage: X_train={X_train.shape}, X_test={X_test.shape}")

# --- DIAGNOSTIC POST-WINDOWING ---
from collections import Counter
print("\nðŸ“Š Distribution aprÃ¨s windowing :")
train_dist = Counter(y_train)
test_dist = Counter(y_test)
print(f"Train: {train_dist}")
print(f"Test: {test_dist}")

# Baseline accuracy
baseline_train = max(train_dist.values()) / sum(train_dist.values())
baseline_test = max(test_dist.values()) / sum(test_dist.values())
print(f"\nðŸ“Œ Baseline (prÃ©dire toujours la classe majoritaire) :")
print(f"   Train: {baseline_train:.1%}")
print(f"   Test: {baseline_test:.1%}")
print(f"   â†’ Le modÃ¨le DOIT battre ce score pour Ãªtre utile !")

# VÃ©rifier l'Ã©quilibre des classes
if baseline_test < 0.75:
    print(f"\nâœ… Distribution Ã©quilibrÃ©e ! Baseline raisonnable ({baseline_test:.1%})")
    print("   â†’ Le modÃ¨le a une vraie chance d'apprendre")
else:
    print(f"\nâš ï¸ Baseline encore Ã©levÃ© ({baseline_test:.1%})")
    print("   â†’ Distribution pas idÃ©ale mais meilleure qu'avant")

# --- CALCUL DES POIDS DE CLASSE ---
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weight_dict = dict(enumerate(class_weights))
print(f"\nâš–ï¸ Poids de classe calculÃ©s: {class_weight_dict}")

# --- DÃ‰FINITION DU MODÃˆLE ---
def StockNetV17(input_shape):
    """
    Architecture optimisÃ©e pour apprendre des patterns relatifs (Top 20%)
    """
    inp = layers.Input(shape=input_shape)
    
    # Bloc 1 - Extraction patterns courts
    x = layers.Conv1D(64, 5, padding="causal", activation="relu")(inp)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.2)(x)
    
    # Bloc 2 - Patterns moyens
    x = layers.Conv1D(128, 3, padding="causal", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.2)(x)
    
    # Bloc 3 - Patterns longs avec dilation
    x = layers.Conv1D(128, 3, padding="causal", dilation_rate=2, activation="relu")(x)
    x = layers.BatchNormalization()(x)
    
    # Attention - Capture les dÃ©pendances temporelles
    attn = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
    x = layers.Add()([x, attn])
    x = layers.LayerNormalization()(x)
    
    # AgrÃ©gation
    x = layers.GlobalAveragePooling1D()(x)
    
    # Classification
    x = layers.Dense(128, activation="relu")(x)
    x = layers.Dropout(0.3)(x)
    x = layers.Dense(64, activation="relu")(x)
    out = layers.Dense(3, activation="softmax")(x)
    
    model = Model(inp, out)
    model.compile(
        optimizer=optimizers.Adam(learning_rate=1e-3),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    
    return model

model = StockNetV17((100, len(features)))
print("\nðŸ§  Architecture du modÃ¨le :")
model.summary()

print("\nâš™ï¸ EntraÃ®nement StockNet v17...")

cb = [
    callbacks.EarlyStopping(patience=7, restore_best_weights=True, verbose=1),
    callbacks.ReduceLROnPlateau(patience=3, factor=0.5, verbose=1, min_lr=1e-6)
]

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=50,
    batch_size=64,
    verbose=1,
    callbacks=cb,
    class_weight=class_weight_dict
)

# --- Ã‰VALUATION ---
loss, acc = model.evaluate(X_test, y_test)
print(f"\nðŸ“Š === RÃ‰SULTATS FINAUX ===")
print(f"Accuracy = {acc:.3f} ({acc:.1%})")
print(f"Baseline = {baseline_test:.3f} ({baseline_test:.1%})")

improvement = (acc - baseline_test) * 100
if acc > baseline_test + 0.05:
    print(f"âœ… SUCCÃˆS ! +{improvement:.1f} points au-dessus du baseline")
    print("   â†’ Le modÃ¨le dÃ©tecte les meilleurs mouvements !")
elif acc > baseline_test:
    print(f"âš ï¸ LÃ©gÃ¨re amÃ©lioration (+{improvement:.1f} points)")
    print("   â†’ Le modÃ¨le apprend mais peut encore progresser")
else:
    print(f"âŒ Baseline non battu (Ã©cart: {improvement:.1f} points)")
    if improvement > -5:
        print("   â†’ Proche du baseline, le modÃ¨le apprend quelque chose")
    else:
        print("   â†’ Le modÃ¨le a du mal Ã  gÃ©nÃ©raliser")

# --- SAUVEGARDE ---
model.save("/home/jx/Documents/stocknet/out/model_v17.keras")
print("\nðŸ’¾ ModÃ¨le sauvegardÃ© : model_v17.keras")

# --- SAUVEGARDE DES PRÃ‰DICTIONS ---
from sklearn.metrics import classification_report, confusion_matrix

print("\nðŸ“ˆ GÃ©nÃ©ration des prÃ©dictions et rapport de test...")
y_pred = np.argmax(model.predict(X_test), axis=1)

# Enregistrement CSV
results = pd.DataFrame({"y_true": y_test, "y_pred": y_pred})
out_path = "/home/jx/Documents/stocknet/out/preds_v17.csv"
results.to_csv(out_path, index=False)
print(f"ðŸ’¾ RÃ©sultats enregistrÃ©s : {out_path}")

# Rapport complet
print("\nðŸ“Š Rapport de classification :")
print(classification_report(y_test, y_pred, digits=3, target_names=["Strong Sell", "Hold", "Strong Buy"]))
print("\nðŸ§© Matrice de confusion :")
print("              Sell  Hold  Buy")
cm = confusion_matrix(y_test, y_pred)
for i, row in enumerate(cm):
    print(f"{['Strong Sell', 'Hold      ', 'Strong Buy '][i]} {row}")

# Analyse des erreurs
print("\nðŸ” Analyse des erreurs :")
total_errors = len(y_test) - np.sum(y_pred == y_test)
print(f"Total erreurs : {total_errors} / {len(y_test)} ({total_errors/len(y_test):.1%})")

# Erreurs critiques
critical_errors = np.sum((y_test == 0) & (y_pred == 2)) + np.sum((y_test == 2) & (y_pred == 0))
print(f"Erreurs critiques (Sellâ†”Buy) : {critical_errors} ({critical_errors/len(y_test):.1%})")

# PrÃ©cision sur les signaux forts uniquement
strong_signals = (y_test != 1)
if strong_signals.sum() > 0:
    strong_acc = np.sum((y_pred == y_test) & strong_signals) / strong_signals.sum()
    print(f"\nðŸ’ª PrÃ©cision sur signaux forts (Buy/Sell) : {strong_acc:.1%}")
    print("   â†’ Mesure la qualitÃ© des prÃ©dictions sur les meilleurs mouvements")
    
    # DÃ©tail par classe
    buy_correct = np.sum((y_pred == 2) & (y_test == 2))
    sell_correct = np.sum((y_pred == 0) & (y_test == 0))
    buy_total = np.sum(y_test == 2)
    sell_total = np.sum(y_test == 0)
    
    if buy_total > 0:
        print(f"   Buy recall : {buy_correct/buy_total:.1%} ({buy_correct}/{buy_total})")
    if sell_total > 0:
        print(f"   Sell recall : {sell_correct/sell_total:.1%} ({sell_correct}/{sell_total})")

print("\nâœ… v17 terminÃ© ! Target adaptatif avec Top 20% ðŸŽ¯")
print("   Distribution 20/60/20 pour meilleur Ã©quilibre !")
