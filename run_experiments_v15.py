#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# run_experiments_v15.py - Target simplifiÃ© 0.15% + Volume confirmation

import os
import sys
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks, Model
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight

# Ajouter les chemins pour les imports locaux
script_dir = os.path.dirname(os.path.abspath(__file__))
code_dir = os.path.join(script_dir, "stocknet-code")
sys.path.insert(0, script_dir)
sys.path.insert(0, code_dir)

# Imports conditionnels des modules locaux
try:
    from vol_price_ratio import calculate_vol_price_ratio
    from ctx_div_cvd_uptrend import calculate_ctx_div_cvd_uptrend
    from ctx_div_rsi_downtrend import calculate_ctx_div_rsi_downtrend
    from ctx_div_cvd_lowvol import calculate_ctx_div_cvd_lowvol
    from ctx_div_rsi_lowvol import calculate_ctx_div_rsi_lowvol
    from cvd_state import calculate_cvd_state
    MODULES_OK = True
except ImportError as e:
    print(f"âš ï¸ Modules locaux non trouvÃ©s: {e}")
    MODULES_OK = False

# --- CONFIG CPU ONLY ---
os.environ["CUDA_VISIBLE_DEVICES"] = ""
tf.get_logger().setLevel("ERROR")

print("\n=== StockNet v15 â€“ Target SimplifiÃ© (0.15% + Volume) ===")

if not MODULES_OK:
    print("\nâŒ Impossible de continuer sans les modules requis.")
    sys.exit(1)

# --- CHARGEMENT DES DONNÃ‰ES ---
df = pd.read_parquet("/home/jx/Documents/stocknet/out/features/BTC_features_v9.parquet")
df = calculate_vol_price_ratio(df)
df = calculate_ctx_div_cvd_uptrend(df)
df = calculate_ctx_div_rsi_downtrend(df)
df = calculate_ctx_div_cvd_lowvol(df)
df = calculate_ctx_div_rsi_lowvol(df)
print(f"âœ… DonnÃ©es : {len(df):,} lignes")

# === INTÃ‰GRATION AUTOMATIQUE DU CVD_STATE ===
df = calculate_cvd_state(df)
print("âœ… Colonne cvd_state gÃ©nÃ©rÃ©e.")

# === AJOUT FEATURE : RÃ‰GIME DE MARCHÃ‰ ===
print("\nðŸŽ¯ Calcul du rÃ©gime de marchÃ©...")

# VolatilitÃ© roulante
rolling_vol = df["close"].pct_change().rolling(50).std()

# Classification en 3 rÃ©gimes
vol_q33 = rolling_vol.quantile(0.33)
vol_q66 = rolling_vol.quantile(0.66)

df["market_regime"] = np.where(
    rolling_vol > vol_q66, 2,      # Haute volatilitÃ© (trending/breakout)
    np.where(rolling_vol < vol_q33, 0, 1)  # Basse volatilitÃ© (range) / Moyen
)

print(f"âœ… RÃ©gimes : Low={vol_q33:.6f}, Mid, High={vol_q66:.6f}")
print(f"Distribution : {df['market_regime'].value_counts().sort_index().to_dict()}")

# === TARGET SIMPLIFIÃ‰ : AMPLITUDE 0.15% + VOLUME ===
print("\nðŸŽ¯ GÃ©nÃ©ration du target simplifiÃ©...")
print("   CritÃ¨res : Mouvement 0.15% + Volume confirmÃ©")
print("   FenÃªtre : 1-12 minutes")

threshold = 0.0015  # 0.15%
window = 12  # minutes

# 1. Calcul des returns futurs sur la fenÃªtre
future_returns = pd.DataFrame({
    f'ret_{i}': df["close"].pct_change(i).shift(-i) 
    for i in range(1, window + 1)
})

# 2. Maximum et minimum des mouvements
max_future_move = future_returns.max(axis=1)
min_future_move = future_returns.min(axis=1)

# 3. VOLUME CONFIRMATION (simplifiÃ© et robuste)
# Calculer le volume moyen pendant le mouvement futur
future_volumes = pd.DataFrame({
    f'vol_{i}': df["volume"].shift(-i) 
    for i in range(1, window + 1)
})

# Volume moyen futur vs volume moyen passÃ©
avg_future_vol = future_volumes.mean(axis=1)
avg_past_vol = df["volume"].rolling(20).mean()
volume_ratio = avg_future_vol / (avg_past_vol + 1e-9)

# Volume confirmÃ© si ratio > 1.15 (15% de volume en plus)
volume_confirmed = (volume_ratio > 1.15).astype(int)

# 4. CLASSIFICATION FINALE (SIMPLE ET ROBUSTE)
# Strong Buy/Sell si :
# - Amplitude > 0.15% 
# - ET Volume confirmÃ© (ratio > 1.15)

buy_signal = (max_future_move > threshold) & (volume_confirmed > 0)
sell_signal = (min_future_move < -threshold) & (volume_confirmed > 0)

df["target"] = np.where(
    buy_signal, 2,
    np.where(sell_signal, 0, 1)
)

# Stockage des mÃ©triques pour analyse
df["max_future_move"] = max_future_move
df["min_future_move"] = min_future_move
df["volume_ratio"] = volume_ratio

df = df.dropna()

# --- DIAGNOSTIC DE DISTRIBUTION ---
print("\nðŸ“Š Distribution des classes (avant split) :")
print(df["target"].value_counts().sort_index())
print("\nProportions :")
print(df["target"].value_counts(normalize=True).sort_index())

# Statistiques dÃ©taillÃ©es
print(f"\nðŸ“ˆ Statistiques des mouvements :")
print(f"   Max moyen : {max_future_move.mean():.4%}")
print(f"   Min moyen : {min_future_move.mean():.4%}")
print(f"   Volume ratio moyen : {volume_ratio.mean():.2f}")
print(f"\n   Mouvements >0.15% : {(max_future_move > threshold).sum():,} ({(max_future_move > threshold).mean():.1%})")
print(f"   Mouvements <-0.15% : {(min_future_move < -threshold).sum():,} ({(min_future_move < -threshold).mean():.1%})")
print(f"   Avec volume confirmÃ© : {(volume_confirmed > 0).sum():,} ({(volume_confirmed > 0).mean():.1%})")
print(f"\n   Buy signals (>0.15% + volume) : {buy_signal.sum():,} ({buy_signal.mean():.1%})")
print(f"   Sell signals (<-0.15% + volume) : {sell_signal.sum():,} ({sell_signal.mean():.1%})")

# --- VÃ‰RIFICATION ANTI-LEAKAGE RENFORCÃ‰E ---
print("\nðŸ” === VÃ‰RIFICATION ANTI-LEAKAGE ===")
print("\nÃ‰chantillon de donnÃ©es (indices 1000-1005) :")
check_cols = ["close", "ret", "rsi_14", "volume_ratio", "target"]
print(df[check_cols].iloc[1000:1005])

# --- LISTE DES FEATURES (avec rÃ©gime de marchÃ©) ---
features = [
    "ret","rsi_14","ema_20","ema_50","volatility_zscore","trend_state",
    "vol_state","ctx_confluence","hour_sin","hour_cos",
    "delta_price","delta_vol","ofi_proxy","cvd_proxy","delta_power",
    "spread","price_change","vol_imbalance","volatility_20","cvd_state",
    "market_regime"
]

print(f"\nâœ… {len(features)} features utilisÃ©es")

# --- SHIFTING ANTI-LEAK STRICT ---
print("\nðŸ”’ Application du shifting anti-leak...")
for f in features:
    if f in df.columns:
        df[f] = df[f].shift(1)
    else:
        print(f"âš ï¸ Feature manquante ignorÃ©e: {f}")
df = df.dropna()

# VÃ©rification post-shift
print("\nðŸ” AprÃ¨s shifting (indices 1000-1005) :")
print(df[check_cols].iloc[1000:1005])

# --- NORMALISATION ---
if "cvd_state" in df.columns:
    mapping = {"Weak": -1, "Neutral": 0, "Strong": 1}
    df["cvd_state"] = df["cvd_state"].map(mapping)
    print("\nâœ… cvd_state encodÃ© numÃ©riquement.")

scaler = StandardScaler()
df[features] = scaler.fit_transform(df[features])

# --- SPLIT TRAIN / TEST ---
split_idx = int(len(df) * 0.75)
train, test = df.iloc[:split_idx], df.iloc[split_idx:]
gap = int(len(df) * 0.05)
test = test.iloc[gap:]

print(f"\nâ±ï¸ Split strict: train={len(train)}, test={len(test)}, gap={gap}")

# --- FENÃŠTRAGE GLISSANT ---
def make_windows(data, horizon=100, stride=10):
    X, y = [], []
    vals, labels = data[features].values, data["target"].values
    for i in range(0, len(vals)-horizon-1, stride):
        X.append(vals[i:i+horizon])
        y.append(labels[i+horizon])
    return np.array(X), np.array(y)

print("\nðŸ”„ FenÃªtrage glissant avec stride=10...")
X_train, y_train = make_windows(train, stride=10)
X_test, y_test = make_windows(test, stride=10)

print(f"âœ… FenÃªtrage: X_train={X_train.shape}, X_test={X_test.shape}")

# --- DIAGNOSTIC POST-WINDOWING ---
from collections import Counter
print("\nðŸ“Š Distribution aprÃ¨s windowing :")
train_dist = Counter(y_train)
test_dist = Counter(y_test)
print(f"Train: {train_dist}")
print(f"Test: {test_dist}")

# Baseline accuracy
baseline_train = max(train_dist.values()) / sum(train_dist.values())
baseline_test = max(test_dist.values()) / sum(test_dist.values())
print(f"\nðŸ“Œ Baseline (prÃ©dire toujours la classe majoritaire) :")
print(f"   Train: {baseline_train:.1%}")
print(f"   Test: {baseline_test:.1%}")
print(f"   â†’ Le modÃ¨le DOIT battre ce score pour Ãªtre utile !")

# VÃ©rifier l'Ã©quilibre des classes
if baseline_test > 0.85:
    print(f"\nâš ï¸ ATTENTION : Baseline trÃ¨s Ã©levÃ© ({baseline_test:.1%})")
    print("   â†’ Les classes sont encore trop dÃ©sÃ©quilibrÃ©es")
    print("   â†’ Le modÃ¨le risque de prÃ©dire toujours Hold")

# --- CALCUL DES POIDS DE CLASSE ---
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weight_dict = dict(enumerate(class_weights))
print(f"\nâš–ï¸ Poids de classe calculÃ©s: {class_weight_dict}")

# --- DÃ‰FINITION DU MODÃˆLE ---
def StockNetV15(input_shape):
    """
    Architecture optimisÃ©e pour dÃ©tecter des mouvements simples et nets
    """
    inp = layers.Input(shape=input_shape)
    
    # Bloc 1 - Extraction patterns courts
    x = layers.Conv1D(64, 5, padding="causal", activation="relu")(inp)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.2)(x)
    
    # Bloc 2 - Patterns moyens
    x = layers.Conv1D(128, 3, padding="causal", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.2)(x)
    
    # Bloc 3 - Patterns longs avec dilation
    x = layers.Conv1D(128, 3, padding="causal", dilation_rate=2, activation="relu")(x)
    x = layers.BatchNormalization()(x)
    
    # Attention - Capture les dÃ©pendances temporelles
    attn = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
    x = layers.Add()([x, attn])
    x = layers.LayerNormalization()(x)
    
    # AgrÃ©gation
    x = layers.GlobalAveragePooling1D()(x)
    
    # Classification
    x = layers.Dense(128, activation="relu")(x)
    x = layers.Dropout(0.3)(x)
    x = layers.Dense(64, activation="relu")(x)
    out = layers.Dense(3, activation="softmax")(x)
    
    model = Model(inp, out)
    model.compile(
        optimizer=optimizers.Adam(learning_rate=1e-3),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    
    return model

model = StockNetV15((100, len(features)))
print("\nðŸ§  Architecture du modÃ¨le :")
model.summary()

print("\nâš™ï¸ EntraÃ®nement StockNet v15...")

cb = [
    callbacks.EarlyStopping(patience=7, restore_best_weights=True, verbose=1),
    callbacks.ReduceLROnPlateau(patience=3, factor=0.5, verbose=1, min_lr=1e-6)
]

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=50,
    batch_size=64,
    verbose=1,
    callbacks=cb,
    class_weight=class_weight_dict
)

# --- Ã‰VALUATION ---
loss, acc = model.evaluate(X_test, y_test)
print(f"\nðŸ“Š === RÃ‰SULTATS FINAUX ===")
print(f"Accuracy = {acc:.3f} ({acc:.1%})")
print(f"Baseline = {baseline_test:.3f} ({baseline_test:.1%})")

improvement = (acc - baseline_test) * 100
if acc > baseline_test + 0.05:
    print(f"âœ… SUCCÃˆS ! +{improvement:.1f} points au-dessus du baseline")
    print("   â†’ Le modÃ¨le dÃ©tecte des mouvements exploitables !")
elif acc > baseline_test:
    print(f"âš ï¸ LÃ©gÃ¨re amÃ©lioration (+{improvement:.1f} points)")
    print("   â†’ Le modÃ¨le apprend mais peut encore progresser")
else:
    print(f"âŒ Ã‰CHEC - Le modÃ¨le n'apprend rien (baseline = {baseline_test:.1%})")
    print("   â†’ Les signaux sont trop faibles ou trop bruitÃ©s")

# --- SAUVEGARDE ---
model.save("/home/jx/Documents/stocknet/out/model_v15.keras")
print("\nðŸ’¾ ModÃ¨le sauvegardÃ© : model_v15.keras")

# --- SAUVEGARDE DES PRÃ‰DICTIONS ---
from sklearn.metrics import classification_report, confusion_matrix

print("\nðŸ“ˆ GÃ©nÃ©ration des prÃ©dictions et rapport de test...")
y_pred = np.argmax(model.predict(X_test), axis=1)

# Enregistrement CSV
results = pd.DataFrame({"y_true": y_test, "y_pred": y_pred})
out_path = "/home/jx/Documents/stocknet/out/preds_v15.csv"
results.to_csv(out_path, index=False)
print(f"ðŸ’¾ RÃ©sultats enregistrÃ©s : {out_path}")

# Rapport complet
print("\nðŸ“Š Rapport de classification :")
print(classification_report(y_test, y_pred, digits=3, target_names=["Strong Sell", "Hold", "Strong Buy"]))
print("\nðŸ§© Matrice de confusion :")
print("              Sell  Hold  Buy")
cm = confusion_matrix(y_test, y_pred)
for i, row in enumerate(cm):
    print(f"{['Strong Sell', 'Hold      ', 'Strong Buy '][i]} {row}")

# Analyse des erreurs
print("\nðŸ” Analyse des erreurs :")
total_errors = len(y_test) - np.sum(y_pred == y_test)
print(f"Total erreurs : {total_errors} / {len(y_test)} ({total_errors/len(y_test):.1%})")

# Erreurs critiques
critical_errors = np.sum((y_test == 0) & (y_pred == 2)) + np.sum((y_test == 2) & (y_pred == 0))
print(f"Erreurs critiques (Sellâ†”Buy) : {critical_errors} ({critical_errors/len(y_test):.1%})")

# PrÃ©cision sur les signaux forts uniquement
strong_signals = (y_test != 1)
if strong_signals.sum() > 0:
    strong_acc = np.sum((y_pred == y_test) & strong_signals) / strong_signals.sum()
    print(f"\nðŸ’ª PrÃ©cision sur signaux forts (Buy/Sell) : {strong_acc:.1%}")
    print("   â†’ Mesure la qualitÃ© rÃ©elle des signaux exploitables")
    
    # DÃ©tail par classe
    buy_correct = np.sum((y_pred == 2) & (y_test == 2))
    sell_correct = np.sum((y_pred == 0) & (y_test == 0))
    buy_total = np.sum(y_test == 2)
    sell_total = np.sum(y_test == 0)
    
    if buy_total > 0:
        print(f"   Buy recall : {buy_correct/buy_total:.1%} ({buy_correct}/{buy_total})")
    if sell_total > 0:
        print(f"   Sell recall : {sell_correct/sell_total:.1%} ({sell_correct}/{sell_total})")

print("\nâœ… v15 terminÃ© ! Target simplifiÃ© 0.15% + Volume ðŸŽ¯")
